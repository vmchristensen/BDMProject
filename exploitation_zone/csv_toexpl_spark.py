from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit, lower, trim
from pyspark.sql.functions import expr
import os

def main():
    spark = SparkSession.builder \
        .appName("CSV_to_Exploitation_By_Disease") \
        .config("spark.jars.packages", "org.apache.hadoop:hadoop-aws:3.3.2,com.amazonaws:aws-java-sdk-bundle:1.11.901") \
        .getOrCreate()

    # AWS config
    hadoop_conf = spark._jsc.hadoopConfiguration()
    hadoop_conf.set("fs.s3a.access.key", os.getenv("AWS_ACCESS_KEY_ID"))
    hadoop_conf.set("fs.s3a.secret.key", os.getenv("AWS_SECRET_ACCESS_KEY"))
    hadoop_conf.set("fs.s3a.endpoint", "s3.amazonaws.com")
    hadoop_conf.set("fs.s3a.path.style.access", "true")
    hadoop_conf.set("fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")

    # === Step 1: Load the CSV file ===
    input_csv_path = "s3a://bdm.trusted.zone/clean_data/csv/gist_mental_health_disorders_unemployment_and_suicides_20250404_203929.csv"
    df = spark.read.option("header", "true").option("inferSchema", "true").csv(input_csv_path)
    # === Step 2: Reshape (pivot to long format) ===
    disorder_columns = [
        "AlcoholUseDisorders", "DrugUseDisorders", "DepressiveDisorders", "BipolarDisorder",
        "AnxietyDisorders", "EatingDisorders", "Schizophrenia"
    ]

    # Use `stack` to melt the wide table into long format
    stack_expr = "stack({}, {}) as (disorder, percentage)".format(
        len(disorder_columns),
        ", ".join([f"'{col_name}', {col_name}" for col_name in disorder_columns])
    )

    long_df = df.select("Entity", "Code", "Year", expr(stack_expr), "TotalPercentageOfPopulation", "Unemployment", "SuicideDeathsRate") \
                .withColumn("disorder", trim(lower(col("disorder"))))  # normalize names

    # === Step 3: Write partitioned CSV to exploitation zone ===
    output_path = "s3a://bdm.exploitation.zone/by_disease/csv/"
    long_df.write.mode("overwrite").partitionBy("disorder").option("header", "true").csv(output_path)

    print("CSV mental health data written to exploitation zone by disorder.")
    spark.stop()

if __name__ == "__main__":
    main()
