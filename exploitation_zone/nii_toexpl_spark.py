from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lower, trim
import os

def main():
    spark = SparkSession.builder \
        .appName("TrustedNii_to_Exploitation_by_Disorder") \
        .config("spark.jars.packages", "org.apache.hadoop:hadoop-aws:3.3.2,com.amazonaws:aws-java-sdk-bundle:1.11.901") \
        .getOrCreate()

    # Configure AWS S3 access
    hadoop_conf = spark._jsc.hadoopConfiguration()
    hadoop_conf.set("fs.s3a.access.key", os.getenv("AWS_ACCESS_KEY_ID"))
    hadoop_conf.set("fs.s3a.secret.key", os.getenv("AWS_SECRET_ACCESS_KEY"))
    hadoop_conf.set("fs.s3a.endpoint", "s3.amazonaws.com")
    hadoop_conf.set("fs.s3a.path.style.access", "true")
    hadoop_conf.set("fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")

    # === Step 1: Read all enriched NII metadata files ===
    input_path = "s3a://bdm.trusted.zone/clean_data/json/nii/*/*.json"
    print(f"Reading metadata from: {input_path}")
    df = spark.read.option("multiline", "true").json(input_path)

    # === Step 2: Normalize disorder field ===
    df = df.withColumn("disorder", trim(lower(col("disorder"))))

    # === Step 3: Write partitioned by disorder to exploitation zone ===
    output_path = "s3a://bdm.exploitation.zone/by_disease/nii/json/"
    print(f"Writing partitioned data to: {output_path}")
    df.write.mode("overwrite").partitionBy("disorder").json(output_path)

    print("Metadata written to exploitation zone, organized by disorder.")
    spark.stop()

if __name__ == "__main__":
    main()
