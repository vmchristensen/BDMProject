from pyspark.sql import SparkSession
from pyspark.sql.functions import col, explode, split, trim, lower
import os

def main():
    spark = SparkSession.builder \
        .appName("Trusted_to_Exploitation_Combined") \
        .config("spark.jars.packages", "org.apache.hadoop:hadoop-aws:3.3.2,com.amazonaws:aws-java-sdk-bundle:1.11.901") \
        .getOrCreate()

    # AWS configuration
    hadoop_conf = spark._jsc.hadoopConfiguration()
    hadoop_conf.set("fs.s3a.access.key", os.getenv("AWS_ACCESS_KEY_ID"))
    hadoop_conf.set("fs.s3a.secret.key", os.getenv("AWS_SECRET_ACCESS_KEY"))
    hadoop_conf.set("fs.s3a.endpoint", "s3.amazonaws.com")
    hadoop_conf.set("fs.s3a.path.style.access", "true")
    hadoop_conf.set("fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")

    # === Step 1: Read JSON data from both PMC and Semantic Scholar ===
    input_paths = [
        "s3a://bdm.trusted.zone/clean_data/json/pmc/spark/json/*.json",
        "s3a://bdm.trusted.zone/clean_data/json/semantic_scholar/json/*.json"
    ]
    df = spark.read.json(input_paths, multiLine=False)
    # df = spark.read.option("multiline", "true").json(input_paths)

    # === Step 2: Clean and explode disorders ===
    df = df.withColumn("disorder_array", split(col("disorders"), ",\\s*")) \
           .withColumn("disorder", explode("disorder_array")) \
           .withColumn("disorder", trim(lower(col("disorder"))))

    # === Step 3: Write to exploitation zone partitioned by disorder ===
    output_path = "s3a://bdm.exploitation.zone/by_disease/json/"
    #df.write.mode("overwrite").partitionBy("disorder").parquet(output_path)
    df.write.mode("overwrite").partitionBy("disorder").json(output_path)

    print("PMC + Semantic Scholar partitioned by disorder and written to exploitation zone.")
    spark.stop()

if __name__ == "__main__":
    main()
